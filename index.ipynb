{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b3c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def9b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95128ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rating': 3.0, 'title': 'Arrived Damaged : liquid in hub locker!', 'text': 'Unfortunately Amazon in their wisdom (cough, cough) decided to ship the snowsuit in a vinyl bag with holes in it!  There was no other bag to protect the snowsuit inside vinyl bag with all the holes.  This is what happened:  Arrived in hub locker. It was the very top locker. Opened it & pulled the pkg out getting a very wet & nasty surprise at the same time. My senses were assaulted. Smells like tea tree oil. Feels like conditioner or lotion.  I can’t understand how the delivery person a) didn’t smell that mess when they shoved the pkg in b) didn’t see the mess when they shoved it in - tho if they were short I guess that would explain it bc I’m 5’10” & I didn’t see it until the pkg was in my hands. The locker was up high & dark, but I could smell it the minute I walked into the hub locker room. I happen to be extremely allergic to tea tree oil.  It’s made from mellaleuca trees which grow all over southwest Florida where I was raised.  Tho native to Australia they were used to help drain the swamps of Florida generations ago bc they soak up so much water.  It has a very distinct smell & unfortunately is frequently used as a “carrier oil” for other oils & body lotions,  shampoos, conditioners, etc., so I have to be hyper-vigilant to avoid exposure.  Instead- Last night I got to suck down Benadryl & pray my throat didn’t close while keeping my epi-pens handy.  The liquid went through the holes & onto the snowsuit as you can clearly see in the pictures.  So I guess the suit is waterproof.  Looks true to size.  Elastic appears stretchy.  I’m super annoyed that this arrived in this condition & could have easily been avoided by amazon properly wrapping it to avoid the elements whatever they may be.', 'images': [{'small_image_url': 'https://m.media-amazon.com/images/I/710WrjJi+hL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/710WrjJi+hL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/710WrjJi+hL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/712+yNg8COL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/712+yNg8COL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/712+yNg8COL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/7132ByALa+L._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/7132ByALa+L._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/7132ByALa+L._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/718OTnxL+CL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/718OTnxL+CL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/718OTnxL+CL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/71I6b8DyazL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/71I6b8DyazL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/71I6b8DyazL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/71v4KKrIXuL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/71v4KKrIXuL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/71v4KKrIXuL._SL1600_.jpg', 'attachment_type': 'IMAGE'}], 'asin': 'B096S6LZV4', 'parent_asin': 'B09NSZ5QMF', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1677938767351, 'helpful_vote': 0, 'verified_purchase': True}\n",
      "{'rating': 3.0, 'title': 'Useless under 40 degrees.', 'text': 'Useless under 40 degrees unless you’re just running to the mailbox & back & don’t mind freezing during that jog.  Zero interior pockets so nowhere to put glasses, iPhone, etc other than the 2 exterior pockets which should be larger imo as they are the only pockets.  Seems to me like they used a too large needle with the thread in mine bc it’s like the air comes through every single hole the thread is in that they used for decoration/insulation to give it a quilted effect.  With cotton fabrics you wash it & the material will shrink around the thread & you don’t have the air coming through the too large holes made by the needle.  I usually stick with the non-quilted versions of non-cotton coats fit this reason.  It looks nice, but it does not keep me warm in northern Colorado where we dip into negative temps & have crazy high winds.  I was not impressed with it. Got it in tye try before you buy program without realizing I would have to pay for bags to return the items I tried or that I would have to drop them off at ups. I’m disabled.  I don’t drive.  So fun to pay for taxis to return stuff when Amazon has a flipping hub at my apt complex.  Why can’t they make returns simpler?  Delivered to the hub & I should be able to return it to the hub imo.  Really ticks me off I don’t get anything delivered to my door anymore since moving to an apt. As a disabled person it’s a massive pain in my tush.  Every step is agony & the stuff weighs more than I am supposed to lift.  It’s a long walk when you’re in chronic pain.  Should have sent it back with the rest bc now I’ll have to pay for another $3 bag & taxis, but I wanted to try it in colder weather since it matched the color of my boots.  My bad.  Useless in cold weather.  The other issue I noticed was I tried 3 Columbia jackets in the same size (xl) bc I like to wear fleece & layers under my coats due to the temps here & the difference in sizing was ridiculous.  This was a true xl while the other two were not.  Easily managed camisole, T-shirt, & long sleeved, very thick, fleece shirt under it.  Just does not keep you warm on its own.  It’s a spring jacket imo not winter.  It has a fuzzy fur inside & I have coke to prefer fleece lined jackets bc I have service dogs & their fur is easier to get off flat fleece than fluffy fur.', 'images': [], 'asin': 'B09KMDBDCN', 'parent_asin': 'B08NGL3X17', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1677083819242, 'helpful_vote': 0, 'verified_purchase': False}\n",
      "{'rating': 4.0, 'title': 'Not waterproof, but a very comfy shoe.', 'text': 'I purchased these bc they are supposed to be waterproof. Mine are not.  Wore them in Colorado in 36 degree weather & I can feel air coming into the toe bed so my toes were cold.  Maybe they didn’t waterproof mine all the way idk or not at all bc mine are not.  I wire regular athletic socks today, but will try them again with wool socks later.  Either way they are one of the most comfy boots I have ever worn. The cushioning is magnificent. I’m disabled & have bad knees, hips, & spine & these were the most comfy shoes I’ve probably had in my life. I say that as an LL Bean, Land’s End, Merrel, Keen, & Columbia long time wearer.  I got the baby blue color for around $40 with tax which is a great deal imo.  The only big issue is I purchased them for winter walks with my dogs so they won’t work for their intended purposes. My dilemma is whether to keep them for spring boots.  The light color means they will get dirty fast so idk.  I just know they are crazy comfy & feel wonderful on my feet. I might try them with some fleece socks too & probably just spray them with boot waterproofing.  They are like a canvas tennis shoe. The material they are made out of.  So super light which I love due to my disabilities, but I don’t think I’ll be wearing these in the back country walks bc of the color I picked. Which I picked solely based on the price. I’m cost effective not cheap lol.  I admit when I got them from the amazon locker they were so incredibly light that I expected garbage.  While these are very light they are also an excellent walking boot imo tho if you need them to be waterproofed good luck bc mine are not.  If you are sticking to plowed roads & sidewalks & avoid big drifts of snow & wear a thick wool sock these will probably work very well if you’re looking for a comfy hiking boot.  Unfortunately I need something that can go off roads into the wilderness & snow drifts bc that’s where my service dogs prefer to walk. Lol.', 'images': [], 'asin': 'B096N5WK8Q', 'parent_asin': 'B07RGM3DYC', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1675524098918, 'helpful_vote': 11, 'verified_purchase': True}\n",
      "Loaded 1,000,000 rows\n",
      "Loaded 2,000,000 rows\n",
      "Loaded 3,000,000 rows\n",
      "Loaded 4,000,000 rows\n",
      "Loaded 5,000,000 rows\n",
      "Loaded 6,000,000 rows\n",
      "Loaded 7,000,000 rows\n",
      "Loaded 8,000,000 rows\n",
      "Loaded 9,000,000 rows\n",
      "Loaded 10,000,000 rows\n",
      "Loaded 11,000,000 rows\n",
      "Loaded 12,000,000 rows\n",
      "Loaded 13,000,000 rows\n",
      "Loaded 14,000,000 rows\n",
      "Loaded 15,000,000 rows\n",
      "Loaded 16,000,000 rows\n",
      "Loaded 17,000,000 rows\n",
      "Loaded 18,000,000 rows\n",
      "Loaded 19,000,000 rows\n",
      "Loaded 20,000,000 rows\n",
      "Loaded 21,000,000 rows\n",
      "Loaded 22,000,000 rows\n",
      "Loaded 23,000,000 rows\n",
      "Loaded 24,000,000 rows\n",
      "Loaded 25,000,000 rows\n",
      "Loaded 26,000,000 rows\n",
      "Loaded 27,000,000 rows\n",
      "Loaded 28,000,000 rows\n",
      "Loaded 29,000,000 rows\n",
      "Loaded 30,000,000 rows\n",
      "Loaded 31,000,000 rows\n",
      "Loaded 32,000,000 rows\n",
      "Loaded 33,000,000 rows\n",
      "Loaded 34,000,000 rows\n",
      "Loaded 35,000,000 rows\n",
      "Loaded 36,000,000 rows\n",
      "Loaded 37,000,000 rows\n",
      "Loaded 38,000,000 rows\n",
      "Loaded 39,000,000 rows\n",
      "Loaded 40,000,000 rows\n",
      "Loaded 41,000,000 rows\n",
      "Loaded 42,000,000 rows\n",
      "Loaded 43,000,000 rows\n",
      "Loaded 44,000,000 rows\n",
      "Loaded 45,000,000 rows\n",
      "Loaded 46,000,000 rows\n",
      "Loaded 47,000,000 rows\n",
      "Loaded 48,000,000 rows\n",
      "Loaded 49,000,000 rows\n",
      "Loaded 50,000,000 rows\n",
      "Loaded 51,000,000 rows\n",
      "Loaded 52,000,000 rows\n",
      "Loaded 53,000,000 rows\n",
      "Loaded 54,000,000 rows\n",
      "Loaded 55,000,000 rows\n",
      "Loaded 56,000,000 rows\n",
      "Loaded 57,000,000 rows\n",
      "Loaded 58,000,000 rows\n",
      "Loaded 59,000,000 rows\n",
      "Loaded 60,000,000 rows\n",
      "Loaded 61,000,000 rows\n",
      "Loaded 62,000,000 rows\n",
      "Loaded 63,000,000 rows\n",
      "Loaded 64,000,000 rows\n",
      "Loaded 65,000,000 rows\n",
      "Loaded 66,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"dataset/Clothing_Shoes_and_Jewelry.jsonl\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        record = json.loads(line)   # one JSON object\n",
    "\n",
    "        # just to confirm it works\n",
    "        if i <= 3:\n",
    "            print(record)\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Loaded {i:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68986b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e88751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1,000,000 reviews\n",
      "Processed 2,000,000 reviews\n",
      "Processed 3,000,000 reviews\n",
      "Processed 4,000,000 reviews\n",
      "Processed 5,000,000 reviews\n",
      "Processed 6,000,000 reviews\n",
      "Processed 7,000,000 reviews\n",
      "Processed 8,000,000 reviews\n",
      "Processed 9,000,000 reviews\n",
      "Processed 10,000,000 reviews\n",
      "Processed 11,000,000 reviews\n",
      "Processed 12,000,000 reviews\n",
      "Processed 13,000,000 reviews\n",
      "Processed 14,000,000 reviews\n",
      "Processed 15,000,000 reviews\n",
      "Processed 16,000,000 reviews\n",
      "Processed 17,000,000 reviews\n",
      "Processed 18,000,000 reviews\n",
      "Processed 19,000,000 reviews\n",
      "Processed 20,000,000 reviews\n",
      "Processed 21,000,000 reviews\n",
      "Processed 22,000,000 reviews\n",
      "Processed 23,000,000 reviews\n",
      "Processed 24,000,000 reviews\n",
      "Processed 25,000,000 reviews\n",
      "Processed 26,000,000 reviews\n",
      "Processed 27,000,000 reviews\n",
      "Processed 28,000,000 reviews\n",
      "Processed 29,000,000 reviews\n",
      "Processed 30,000,000 reviews\n",
      "Processed 31,000,000 reviews\n",
      "Processed 32,000,000 reviews\n",
      "Processed 33,000,000 reviews\n",
      "Processed 34,000,000 reviews\n",
      "Processed 35,000,000 reviews\n",
      "Processed 36,000,000 reviews\n",
      "Processed 37,000,000 reviews\n",
      "Processed 38,000,000 reviews\n",
      "Processed 39,000,000 reviews\n",
      "Processed 40,000,000 reviews\n",
      "Processed 41,000,000 reviews\n",
      "Processed 42,000,000 reviews\n",
      "Processed 43,000,000 reviews\n",
      "Processed 44,000,000 reviews\n",
      "Processed 45,000,000 reviews\n",
      "Processed 46,000,000 reviews\n",
      "Processed 47,000,000 reviews\n",
      "Processed 48,000,000 reviews\n",
      "Processed 49,000,000 reviews\n",
      "Processed 50,000,000 reviews\n",
      "Processed 51,000,000 reviews\n",
      "Processed 52,000,000 reviews\n",
      "Processed 53,000,000 reviews\n",
      "Processed 54,000,000 reviews\n",
      "Processed 55,000,000 reviews\n",
      "Processed 56,000,000 reviews\n",
      "Processed 57,000,000 reviews\n",
      "Processed 58,000,000 reviews\n",
      "Processed 59,000,000 reviews\n",
      "Processed 60,000,000 reviews\n",
      "Processed 61,000,000 reviews\n",
      "Processed 62,000,000 reviews\n",
      "Processed 63,000,000 reviews\n",
      "Processed 64,000,000 reviews\n",
      "Processed 65,000,000 reviews\n",
      "Processed 66,000,000 reviews\n",
      "Saved: dataset/user_labels.csv\n",
      "Total users labeled: 22553370\n"
     ]
    }
   ],
   "source": [
    "out_csv = \"dataset/user_labels.csv\"\n",
    "\n",
    "total_reviews = defaultdict(int)\n",
    "helpful_reviews = defaultdict(int)\n",
    "\n",
    "# 1) PASS: build user counts\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        r = json.loads(line)\n",
    "        u = r.get(\"user_id\")\n",
    "        if not u:\n",
    "            continue\n",
    "\n",
    "        total_reviews[u] += 1\n",
    "\n",
    "        # thesis rule: helpful review if helpful_vote > 5\n",
    "        if int(r.get(\"helpful_vote\", 0)) > 5:\n",
    "            helpful_reviews[u] += 1\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Processed {i:,} reviews\")\n",
    "\n",
    "# 2) Create labels + save\n",
    "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"user_id\", \"total_reviews\", \"helpful_reviews\", \"Ru\", \"label\"])\n",
    "\n",
    "    for u, tot in total_reviews.items():\n",
    "        hel = helpful_reviews[u]\n",
    "        Ru = hel / tot\n",
    "\n",
    "        if Ru >= 0.7:\n",
    "            label = \"genuine\"\n",
    "        elif Ru <= 0.3:\n",
    "            label = \"fake\"\n",
    "        else:\n",
    "            label = \"unlabeled\"\n",
    "\n",
    "        writer.writerow([u, tot, hel, Ru, label])\n",
    "\n",
    "print(\"Saved:\", out_csv)\n",
    "print(\"Total users labeled:\", len(total_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10196064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>total_reviews</th>\n",
       "      <th>helpful_reviews</th>\n",
       "      <th>Ru</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGGZ357AO26RQZVRLGU4D4N52DZQ</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGKASBHYZPGTEPO6LWZPVJWB2BVA</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AG2L7H23R5LLKDKLBEF2Q3L2MVDA</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGCI7FAH4GL5FI65HYLKWTMFZ2CQ</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id  total_reviews  helpful_reviews   Ru  \\\n",
       "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ              4                2  0.5   \n",
       "1  AGGZ357AO26RQZVRLGU4D4N52DZQ             10                3  0.3   \n",
       "2  AGKASBHYZPGTEPO6LWZPVJWB2BVA             15                0  0.0   \n",
       "3  AG2L7H23R5LLKDKLBEF2Q3L2MVDA             10                0  0.0   \n",
       "4  AGCI7FAH4GL5FI65HYLKWTMFZ2CQ              7                0  0.0   \n",
       "\n",
       "       label  \n",
       "0  unlabeled  \n",
       "1       fake  \n",
       "2       fake  \n",
       "3       fake  \n",
       "4       fake  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_labels = pd.read_csv(\"dataset/user_labels.csv\")\n",
    "df_labels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89396bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded users from CSV: 22553370\n",
      "Processed 1,000,000 rows | missing users: 0\n",
      "Processed 2,000,000 rows | missing users: 0\n",
      "Processed 3,000,000 rows | missing users: 0\n",
      "Processed 4,000,000 rows | missing users: 0\n",
      "Processed 5,000,000 rows | missing users: 0\n",
      "Processed 6,000,000 rows | missing users: 0\n",
      "Processed 7,000,000 rows | missing users: 0\n",
      "Processed 8,000,000 rows | missing users: 0\n",
      "Processed 9,000,000 rows | missing users: 0\n",
      "Processed 10,000,000 rows | missing users: 0\n",
      "Processed 11,000,000 rows | missing users: 0\n",
      "Processed 12,000,000 rows | missing users: 0\n",
      "Processed 13,000,000 rows | missing users: 0\n",
      "Processed 14,000,000 rows | missing users: 0\n",
      "Processed 15,000,000 rows | missing users: 0\n",
      "Processed 16,000,000 rows | missing users: 0\n",
      "Processed 17,000,000 rows | missing users: 0\n",
      "Processed 18,000,000 rows | missing users: 0\n",
      "Processed 19,000,000 rows | missing users: 0\n",
      "Processed 20,000,000 rows | missing users: 0\n",
      "Processed 21,000,000 rows | missing users: 0\n",
      "Processed 22,000,000 rows | missing users: 0\n",
      "Processed 23,000,000 rows | missing users: 0\n",
      "Processed 24,000,000 rows | missing users: 0\n",
      "Processed 25,000,000 rows | missing users: 0\n",
      "Processed 26,000,000 rows | missing users: 0\n",
      "Processed 27,000,000 rows | missing users: 0\n",
      "Processed 28,000,000 rows | missing users: 0\n",
      "Processed 29,000,000 rows | missing users: 0\n",
      "Processed 30,000,000 rows | missing users: 0\n",
      "Processed 31,000,000 rows | missing users: 0\n",
      "Processed 32,000,000 rows | missing users: 0\n",
      "Processed 33,000,000 rows | missing users: 0\n",
      "Processed 34,000,000 rows | missing users: 0\n",
      "Processed 35,000,000 rows | missing users: 0\n",
      "Processed 36,000,000 rows | missing users: 0\n",
      "Processed 37,000,000 rows | missing users: 0\n",
      "Processed 38,000,000 rows | missing users: 0\n",
      "Processed 39,000,000 rows | missing users: 0\n",
      "Processed 40,000,000 rows | missing users: 0\n",
      "Processed 41,000,000 rows | missing users: 0\n",
      "Processed 42,000,000 rows | missing users: 0\n",
      "Processed 43,000,000 rows | missing users: 0\n",
      "Processed 44,000,000 rows | missing users: 0\n",
      "Processed 45,000,000 rows | missing users: 0\n",
      "Processed 46,000,000 rows | missing users: 0\n",
      "Processed 47,000,000 rows | missing users: 0\n",
      "Processed 48,000,000 rows | missing users: 0\n",
      "Processed 49,000,000 rows | missing users: 0\n",
      "Processed 50,000,000 rows | missing users: 0\n",
      "Processed 51,000,000 rows | missing users: 0\n",
      "Processed 52,000,000 rows | missing users: 0\n",
      "Processed 53,000,000 rows | missing users: 0\n",
      "Processed 54,000,000 rows | missing users: 0\n",
      "Processed 55,000,000 rows | missing users: 0\n",
      "Processed 56,000,000 rows | missing users: 0\n",
      "Processed 57,000,000 rows | missing users: 0\n",
      "Processed 58,000,000 rows | missing users: 0\n",
      "Processed 59,000,000 rows | missing users: 0\n",
      "Processed 60,000,000 rows | missing users: 0\n",
      "Processed 61,000,000 rows | missing users: 0\n",
      "Processed 62,000,000 rows | missing users: 0\n",
      "Processed 63,000,000 rows | missing users: 0\n",
      "Processed 64,000,000 rows | missing users: 0\n",
      "Processed 65,000,000 rows | missing users: 0\n",
      "Processed 66,000,000 rows | missing users: 0\n",
      "Done.\n",
      "Output: dataset/Clothing_Shoes_and_Jewelry_with_labels.jsonl\n",
      "Total rows written: 66033346\n",
      "Rows with missing label: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels_csv = \"dataset/user_labels.csv\"\n",
    "input_jsonl = \"dataset/Clothing_Shoes_and_Jewelry.jsonl\"\n",
    "output_jsonl = \"dataset/Clothing_Shoes_and_Jewelry_with_labels.jsonl\"\n",
    "\n",
    "# 1) Load user labels into memory (dict)\n",
    "user2lab = {}\n",
    "with open(labels_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        u = row[\"user_id\"]\n",
    "        Ru = float(row[\"Ru\"])\n",
    "        label = row[\"label\"]\n",
    "        user2lab[u] = (Ru, label)\n",
    "\n",
    "print(\"Loaded users from CSV:\", len(user2lab))\n",
    "\n",
    "# 2) Stream JSONL and write enriched JSONL\n",
    "missing = 0\n",
    "written = 0\n",
    "\n",
    "with open(input_jsonl, \"r\", encoding=\"utf-8\") as fin, open(output_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for i, line in enumerate(fin, 1):\n",
    "        r = json.loads(line)\n",
    "\n",
    "        u = r.get(\"user_id\")\n",
    "        if u in user2lab:\n",
    "            Ru, label = user2lab[u]\n",
    "            r[\"Ru\"] = Ru\n",
    "            r[\"label\"] = label\n",
    "        else:\n",
    "            # If some user_id isn't present in CSV\n",
    "            r[\"Ru\"] = None\n",
    "            r[\"label\"] = None\n",
    "            missing += 1\n",
    "\n",
    "        fout.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "        written += 1\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Processed {i:,} rows | missing users: {missing:,}\")\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Output:\", output_jsonl)\n",
    "print(\"Total rows written:\", written)\n",
    "print(\"Rows with missing label:\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5ffe020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rating': 3.0, 'title': 'Arrived Damaged : liquid in hub locker!', 'text': 'Unfortunately Amazon in their wisdom (cough, cough) decided to ship the snowsuit in a vinyl bag with holes in it!  There was no other bag to protect the snowsuit inside vinyl bag with all the holes.  This is what happened:  Arrived in hub locker. It was the very top locker. Opened it & pulled the pkg out getting a very wet & nasty surprise at the same time. My senses were assaulted. Smells like tea tree oil. Feels like conditioner or lotion.  I can’t understand how the delivery person a) didn’t smell that mess when they shoved the pkg in b) didn’t see the mess when they shoved it in - tho if they were short I guess that would explain it bc I’m 5’10” & I didn’t see it until the pkg was in my hands. The locker was up high & dark, but I could smell it the minute I walked into the hub locker room. I happen to be extremely allergic to tea tree oil.  It’s made from mellaleuca trees which grow all over southwest Florida where I was raised.  Tho native to Australia they were used to help drain the swamps of Florida generations ago bc they soak up so much water.  It has a very distinct smell & unfortunately is frequently used as a “carrier oil” for other oils & body lotions,  shampoos, conditioners, etc., so I have to be hyper-vigilant to avoid exposure.  Instead- Last night I got to suck down Benadryl & pray my throat didn’t close while keeping my epi-pens handy.  The liquid went through the holes & onto the snowsuit as you can clearly see in the pictures.  So I guess the suit is waterproof.  Looks true to size.  Elastic appears stretchy.  I’m super annoyed that this arrived in this condition & could have easily been avoided by amazon properly wrapping it to avoid the elements whatever they may be.', 'images': [{'small_image_url': 'https://m.media-amazon.com/images/I/710WrjJi+hL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/710WrjJi+hL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/710WrjJi+hL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/712+yNg8COL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/712+yNg8COL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/712+yNg8COL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/7132ByALa+L._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/7132ByALa+L._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/7132ByALa+L._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/718OTnxL+CL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/718OTnxL+CL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/718OTnxL+CL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/71I6b8DyazL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/71I6b8DyazL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/71I6b8DyazL._SL1600_.jpg', 'attachment_type': 'IMAGE'}, {'small_image_url': 'https://m.media-amazon.com/images/I/71v4KKrIXuL._SL256_.jpg', 'medium_image_url': 'https://m.media-amazon.com/images/I/71v4KKrIXuL._SL800_.jpg', 'large_image_url': 'https://m.media-amazon.com/images/I/71v4KKrIXuL._SL1600_.jpg', 'attachment_type': 'IMAGE'}], 'asin': 'B096S6LZV4', 'parent_asin': 'B09NSZ5QMF', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1677938767351, 'helpful_vote': 0, 'verified_purchase': True, 'Ru': 0.5, 'label': 'unlabeled'}\n",
      "{'rating': 3.0, 'title': 'Useless under 40 degrees.', 'text': 'Useless under 40 degrees unless you’re just running to the mailbox & back & don’t mind freezing during that jog.  Zero interior pockets so nowhere to put glasses, iPhone, etc other than the 2 exterior pockets which should be larger imo as they are the only pockets.  Seems to me like they used a too large needle with the thread in mine bc it’s like the air comes through every single hole the thread is in that they used for decoration/insulation to give it a quilted effect.  With cotton fabrics you wash it & the material will shrink around the thread & you don’t have the air coming through the too large holes made by the needle.  I usually stick with the non-quilted versions of non-cotton coats fit this reason.  It looks nice, but it does not keep me warm in northern Colorado where we dip into negative temps & have crazy high winds.  I was not impressed with it. Got it in tye try before you buy program without realizing I would have to pay for bags to return the items I tried or that I would have to drop them off at ups. I’m disabled.  I don’t drive.  So fun to pay for taxis to return stuff when Amazon has a flipping hub at my apt complex.  Why can’t they make returns simpler?  Delivered to the hub & I should be able to return it to the hub imo.  Really ticks me off I don’t get anything delivered to my door anymore since moving to an apt. As a disabled person it’s a massive pain in my tush.  Every step is agony & the stuff weighs more than I am supposed to lift.  It’s a long walk when you’re in chronic pain.  Should have sent it back with the rest bc now I’ll have to pay for another $3 bag & taxis, but I wanted to try it in colder weather since it matched the color of my boots.  My bad.  Useless in cold weather.  The other issue I noticed was I tried 3 Columbia jackets in the same size (xl) bc I like to wear fleece & layers under my coats due to the temps here & the difference in sizing was ridiculous.  This was a true xl while the other two were not.  Easily managed camisole, T-shirt, & long sleeved, very thick, fleece shirt under it.  Just does not keep you warm on its own.  It’s a spring jacket imo not winter.  It has a fuzzy fur inside & I have coke to prefer fleece lined jackets bc I have service dogs & their fur is easier to get off flat fleece than fluffy fur.', 'images': [], 'asin': 'B09KMDBDCN', 'parent_asin': 'B08NGL3X17', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1677083819242, 'helpful_vote': 0, 'verified_purchase': False, 'Ru': 0.5, 'label': 'unlabeled'}\n",
      "{'rating': 4.0, 'title': 'Not waterproof, but a very comfy shoe.', 'text': 'I purchased these bc they are supposed to be waterproof. Mine are not.  Wore them in Colorado in 36 degree weather & I can feel air coming into the toe bed so my toes were cold.  Maybe they didn’t waterproof mine all the way idk or not at all bc mine are not.  I wire regular athletic socks today, but will try them again with wool socks later.  Either way they are one of the most comfy boots I have ever worn. The cushioning is magnificent. I’m disabled & have bad knees, hips, & spine & these were the most comfy shoes I’ve probably had in my life. I say that as an LL Bean, Land’s End, Merrel, Keen, & Columbia long time wearer.  I got the baby blue color for around $40 with tax which is a great deal imo.  The only big issue is I purchased them for winter walks with my dogs so they won’t work for their intended purposes. My dilemma is whether to keep them for spring boots.  The light color means they will get dirty fast so idk.  I just know they are crazy comfy & feel wonderful on my feet. I might try them with some fleece socks too & probably just spray them with boot waterproofing.  They are like a canvas tennis shoe. The material they are made out of.  So super light which I love due to my disabilities, but I don’t think I’ll be wearing these in the back country walks bc of the color I picked. Which I picked solely based on the price. I’m cost effective not cheap lol.  I admit when I got them from the amazon locker they were so incredibly light that I expected garbage.  While these are very light they are also an excellent walking boot imo tho if you need them to be waterproofed good luck bc mine are not.  If you are sticking to plowed roads & sidewalks & avoid big drifts of snow & wear a thick wool sock these will probably work very well if you’re looking for a comfy hiking boot.  Unfortunately I need something that can go off roads into the wilderness & snow drifts bc that’s where my service dogs prefer to walk. Lol.', 'images': [], 'asin': 'B096N5WK8Q', 'parent_asin': 'B07RGM3DYC', 'user_id': 'AFKZENTNBQ7A7V7UXW5JJI6UGRYQ', 'timestamp': 1675524098918, 'helpful_vote': 11, 'verified_purchase': True, 'Ru': 0.5, 'label': 'unlabeled'}\n",
      "Loaded 1,000,000 rows\n",
      "Loaded 2,000,000 rows\n",
      "Loaded 3,000,000 rows\n",
      "Loaded 4,000,000 rows\n",
      "Loaded 5,000,000 rows\n",
      "Loaded 6,000,000 rows\n",
      "Loaded 7,000,000 rows\n",
      "Loaded 8,000,000 rows\n",
      "Loaded 9,000,000 rows\n",
      "Loaded 10,000,000 rows\n",
      "Loaded 11,000,000 rows\n",
      "Loaded 12,000,000 rows\n",
      "Loaded 13,000,000 rows\n",
      "Loaded 14,000,000 rows\n",
      "Loaded 15,000,000 rows\n",
      "Loaded 16,000,000 rows\n",
      "Loaded 17,000,000 rows\n",
      "Loaded 18,000,000 rows\n",
      "Loaded 19,000,000 rows\n",
      "Loaded 20,000,000 rows\n",
      "Loaded 21,000,000 rows\n",
      "Loaded 22,000,000 rows\n",
      "Loaded 23,000,000 rows\n",
      "Loaded 24,000,000 rows\n",
      "Loaded 25,000,000 rows\n",
      "Loaded 26,000,000 rows\n",
      "Loaded 27,000,000 rows\n",
      "Loaded 28,000,000 rows\n",
      "Loaded 29,000,000 rows\n",
      "Loaded 30,000,000 rows\n",
      "Loaded 31,000,000 rows\n",
      "Loaded 32,000,000 rows\n",
      "Loaded 33,000,000 rows\n",
      "Loaded 34,000,000 rows\n",
      "Loaded 35,000,000 rows\n",
      "Loaded 36,000,000 rows\n",
      "Loaded 37,000,000 rows\n",
      "Loaded 38,000,000 rows\n",
      "Loaded 39,000,000 rows\n",
      "Loaded 40,000,000 rows\n",
      "Loaded 41,000,000 rows\n",
      "Loaded 42,000,000 rows\n",
      "Loaded 43,000,000 rows\n",
      "Loaded 44,000,000 rows\n",
      "Loaded 45,000,000 rows\n",
      "Loaded 46,000,000 rows\n",
      "Loaded 47,000,000 rows\n",
      "Loaded 48,000,000 rows\n",
      "Loaded 49,000,000 rows\n",
      "Loaded 50,000,000 rows\n",
      "Loaded 51,000,000 rows\n",
      "Loaded 52,000,000 rows\n",
      "Loaded 53,000,000 rows\n",
      "Loaded 54,000,000 rows\n",
      "Loaded 55,000,000 rows\n",
      "Loaded 56,000,000 rows\n",
      "Loaded 57,000,000 rows\n",
      "Loaded 58,000,000 rows\n",
      "Loaded 59,000,000 rows\n",
      "Loaded 60,000,000 rows\n",
      "Loaded 61,000,000 rows\n",
      "Loaded 62,000,000 rows\n",
      "Loaded 63,000,000 rows\n",
      "Loaded 64,000,000 rows\n",
      "Loaded 65,000,000 rows\n",
      "Loaded 66,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "label_path = \"dataset/Clothing_Shoes_and_Jewelry_with_labels.jsonl\"\n",
    "\n",
    "with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        record = json.loads(line)   # one JSON object\n",
    "\n",
    "        # just to confirm it works\n",
    "        if i <= 3:\n",
    "            print(record)\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Loaded {i:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0a6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9439fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1,000,000 reviews\n",
      "Processed 2,000,000 reviews\n",
      "Processed 3,000,000 reviews\n",
      "Processed 4,000,000 reviews\n",
      "Processed 5,000,000 reviews\n",
      "Processed 6,000,000 reviews\n",
      "Processed 7,000,000 reviews\n",
      "Processed 8,000,000 reviews\n",
      "Processed 9,000,000 reviews\n",
      "Processed 10,000,000 reviews\n",
      "Processed 11,000,000 reviews\n",
      "Processed 12,000,000 reviews\n",
      "Processed 13,000,000 reviews\n",
      "Processed 14,000,000 reviews\n",
      "Processed 15,000,000 reviews\n",
      "Processed 16,000,000 reviews\n",
      "Processed 17,000,000 reviews\n",
      "Processed 18,000,000 reviews\n",
      "Processed 19,000,000 reviews\n",
      "Processed 20,000,000 reviews\n",
      "Processed 21,000,000 reviews\n",
      "Processed 22,000,000 reviews\n",
      "Processed 23,000,000 reviews\n",
      "Processed 24,000,000 reviews\n",
      "Processed 25,000,000 reviews\n",
      "Processed 26,000,000 reviews\n",
      "Processed 27,000,000 reviews\n",
      "Processed 28,000,000 reviews\n",
      "Processed 29,000,000 reviews\n",
      "Processed 30,000,000 reviews\n",
      "Processed 31,000,000 reviews\n",
      "Processed 32,000,000 reviews\n",
      "Processed 33,000,000 reviews\n",
      "Processed 34,000,000 reviews\n",
      "Processed 35,000,000 reviews\n",
      "Processed 36,000,000 reviews\n",
      "Processed 37,000,000 reviews\n",
      "Processed 38,000,000 reviews\n",
      "Processed 39,000,000 reviews\n",
      "Processed 40,000,000 reviews\n",
      "Processed 41,000,000 reviews\n",
      "Processed 42,000,000 reviews\n",
      "Processed 43,000,000 reviews\n",
      "Processed 44,000,000 reviews\n",
      "Processed 45,000,000 reviews\n",
      "Processed 46,000,000 reviews\n",
      "Processed 47,000,000 reviews\n",
      "Processed 48,000,000 reviews\n",
      "Processed 49,000,000 reviews\n",
      "Processed 50,000,000 reviews\n",
      "Processed 51,000,000 reviews\n",
      "Processed 52,000,000 reviews\n",
      "Processed 53,000,000 reviews\n",
      "Processed 54,000,000 reviews\n",
      "Processed 55,000,000 reviews\n",
      "Processed 56,000,000 reviews\n",
      "Processed 57,000,000 reviews\n",
      "Processed 58,000,000 reviews\n",
      "Processed 59,000,000 reviews\n",
      "Processed 60,000,000 reviews\n",
      "Processed 61,000,000 reviews\n",
      "Processed 62,000,000 reviews\n",
      "Processed 63,000,000 reviews\n",
      "Processed 64,000,000 reviews\n",
      "Processed 65,000,000 reviews\n",
      "Processed 66,000,000 reviews\n",
      "Item means computed: 15229158\n",
      "Global avg length: 31.6846331397473\n",
      "PASS2 processed 1,000,000 reviews\n",
      "PASS2 processed 2,000,000 reviews\n",
      "PASS2 processed 3,000,000 reviews\n",
      "PASS2 processed 4,000,000 reviews\n",
      "PASS2 processed 5,000,000 reviews\n",
      "PASS2 processed 6,000,000 reviews\n",
      "PASS2 processed 7,000,000 reviews\n",
      "PASS2 processed 8,000,000 reviews\n",
      "PASS2 processed 9,000,000 reviews\n",
      "PASS2 processed 10,000,000 reviews\n",
      "PASS2 processed 11,000,000 reviews\n",
      "PASS2 processed 12,000,000 reviews\n",
      "PASS2 processed 13,000,000 reviews\n",
      "PASS2 processed 14,000,000 reviews\n",
      "PASS2 processed 15,000,000 reviews\n",
      "PASS2 processed 16,000,000 reviews\n",
      "PASS2 processed 17,000,000 reviews\n",
      "PASS2 processed 18,000,000 reviews\n",
      "PASS2 processed 19,000,000 reviews\n",
      "PASS2 processed 20,000,000 reviews\n",
      "PASS2 processed 21,000,000 reviews\n",
      "PASS2 processed 22,000,000 reviews\n",
      "PASS2 processed 23,000,000 reviews\n",
      "PASS2 processed 24,000,000 reviews\n",
      "PASS2 processed 25,000,000 reviews\n",
      "PASS2 processed 26,000,000 reviews\n",
      "PASS2 processed 27,000,000 reviews\n",
      "PASS2 processed 28,000,000 reviews\n",
      "PASS2 processed 29,000,000 reviews\n",
      "PASS2 processed 30,000,000 reviews\n",
      "PASS2 processed 31,000,000 reviews\n",
      "PASS2 processed 32,000,000 reviews\n",
      "PASS2 processed 33,000,000 reviews\n",
      "PASS2 processed 34,000,000 reviews\n",
      "PASS2 processed 35,000,000 reviews\n",
      "PASS2 processed 36,000,000 reviews\n",
      "PASS2 processed 37,000,000 reviews\n",
      "PASS2 processed 38,000,000 reviews\n",
      "PASS2 processed 39,000,000 reviews\n",
      "PASS2 processed 40,000,000 reviews\n",
      "PASS2 processed 41,000,000 reviews\n",
      "PASS2 processed 42,000,000 reviews\n",
      "PASS2 processed 43,000,000 reviews\n",
      "PASS2 processed 44,000,000 reviews\n",
      "PASS2 processed 45,000,000 reviews\n",
      "PASS2 processed 46,000,000 reviews\n",
      "PASS2 processed 47,000,000 reviews\n",
      "PASS2 processed 48,000,000 reviews\n",
      "PASS2 processed 49,000,000 reviews\n",
      "PASS2 processed 50,000,000 reviews\n",
      "PASS2 processed 51,000,000 reviews\n",
      "PASS2 processed 52,000,000 reviews\n",
      "PASS2 processed 53,000,000 reviews\n",
      "PASS2 processed 54,000,000 reviews\n",
      "PASS2 processed 55,000,000 reviews\n",
      "PASS2 processed 56,000,000 reviews\n",
      "PASS2 processed 57,000,000 reviews\n",
      "PASS2 processed 58,000,000 reviews\n",
      "PASS2 processed 59,000,000 reviews\n",
      "PASS2 processed 60,000,000 reviews\n",
      "PASS2 processed 61,000,000 reviews\n",
      "PASS2 processed 62,000,000 reviews\n",
      "PASS2 processed 63,000,000 reviews\n",
      "PASS2 processed 64,000,000 reviews\n",
      "PASS2 processed 65,000,000 reviews\n",
      "PASS2 processed 66,000,000 reviews\n",
      "Saved features to: dataset/user_features.csv\n",
      "Users: 22553370\n"
     ]
    }
   ],
   "source": [
    "JSONL_PATH = \"dataset/Clothing_Shoes_and_Jewelry_with_labels.jsonl\"\n",
    "OUT_CSV = \"dataset/user_features.csv\"\n",
    "\n",
    "TAU_MS = 24 * 60 * 60 * 1000  # 1 day (adjust)\n",
    "\n",
    "token_re = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    return token_re.findall(text.lower())\n",
    "\n",
    "# -------------------------\n",
    "# PASS 1: collect aggregates\n",
    "# -------------------------\n",
    "\n",
    "# user aggregates\n",
    "user_n = defaultdict(int)\n",
    "user_r = defaultdict(lambda: [0,0,0,0,0])          # rating counts 1..5\n",
    "user_extreme = defaultdict(int)                    # count ratings in {1,5}\n",
    "user_words_sum = defaultdict(int)                  # total words\n",
    "user_ttr_sum = defaultdict(float)                  # sum of per-review TTR (proxy for LD)\n",
    "user_bucket_cnt = defaultdict(lambda: defaultdict(int))  # (user -> bucket -> count) for burst approx\n",
    "\n",
    "# Ru/label lookup (already inside JSONL, but we store one copy per user)\n",
    "user_Ru = {}\n",
    "user_label = {}\n",
    "\n",
    "# item aggregates for item mean r̄_i (for AAD)\n",
    "item_sum = defaultdict(float)\n",
    "item_cnt = defaultdict(int)\n",
    "\n",
    "# global mean review length ℓ̄\n",
    "global_len_sum = 0\n",
    "global_len_cnt = 0\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        r = json.loads(line)\n",
    "\n",
    "        uid = r.get(\"user_id\")\n",
    "        asin = r.get(\"asin\")\n",
    "        rating = r.get(\"rating\")\n",
    "        if not uid or not asin or rating is None:\n",
    "            continue\n",
    "\n",
    "        # store Ru/label once (they are already in your jsonl)\n",
    "        if uid not in user_Ru:\n",
    "            user_Ru[uid] = r.get(\"Ru\", None)\n",
    "            user_label[uid] = r.get(\"label\", None)\n",
    "\n",
    "        ri = int(round(float(rating)))\n",
    "        ri = 1 if ri < 1 else 5 if ri > 5 else ri\n",
    "\n",
    "        user_n[uid] += 1\n",
    "        user_r[uid][ri-1] += 1\n",
    "        if ri in (1, 5):\n",
    "            user_extreme[uid] += 1\n",
    "\n",
    "        # item mean stats (for AAD)\n",
    "        item_sum[asin] += ri\n",
    "        item_cnt[asin] += 1\n",
    "\n",
    "        # text length + lexical proxy\n",
    "        text = (r.get(\"title\") or \"\") + \" \" + (r.get(\"text\") or \"\")\n",
    "        toks = tokenize(text)\n",
    "        L = len(toks)\n",
    "\n",
    "        user_words_sum[uid] += L\n",
    "        global_len_sum += L\n",
    "        global_len_cnt += 1\n",
    "\n",
    "        # LD proxy = per-review type-token ratio\n",
    "        if L > 0:\n",
    "            user_ttr_sum[uid] += len(set(toks)) / L\n",
    "\n",
    "        # Burst approx by τ-bucket\n",
    "        ts = r.get(\"timestamp\")\n",
    "        if ts is not None:\n",
    "            try:\n",
    "                bucket = int(int(ts) // TAU_MS)\n",
    "                user_bucket_cnt[uid][bucket] += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Processed {i:,} reviews\")\n",
    "\n",
    "global_avg_len = global_len_sum / max(global_len_cnt, 1)\n",
    "\n",
    "# compute item means\n",
    "item_mean = {a: item_sum[a]/item_cnt[a] for a in item_cnt}\n",
    "print(\"Item means computed:\", len(item_mean))\n",
    "print(\"Global avg length:\", global_avg_len)\n",
    "\n",
    "# -------------------------\n",
    "# PASS 2: compute AAD & RD sums per user\n",
    "# -------------------------\n",
    "user_aad_sum = defaultdict(float)\n",
    "user_rd_sum = defaultdict(float)\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        r = json.loads(line)\n",
    "\n",
    "        uid = r.get(\"user_id\")\n",
    "        asin = r.get(\"asin\")\n",
    "        rating = r.get(\"rating\")\n",
    "        if not uid or not asin or rating is None:\n",
    "            continue\n",
    "\n",
    "        ri = int(round(float(rating)))\n",
    "        ri = 1 if ri < 1 else 5 if ri > 5 else ri\n",
    "\n",
    "        rbar = item_mean.get(asin)\n",
    "        if rbar is None:\n",
    "            continue\n",
    "\n",
    "        user_aad_sum[uid] += abs(ri - rbar)\n",
    "\n",
    "        text = (r.get(\"title\") or \"\") + \" \" + (r.get(\"text\") or \"\")\n",
    "        L = len(tokenize(text))\n",
    "        user_rd_sum[uid] += abs(L - global_avg_len)\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"PASS2 processed {i:,} reviews\")\n",
    "\n",
    "# -------------------------\n",
    "# Final: compute 6 features + save\n",
    "# -------------------------\n",
    "def entropy(counts):\n",
    "    n = sum(counts)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    H = 0.0\n",
    "    for c in counts:\n",
    "        if c > 0:\n",
    "            p = c / n\n",
    "            H -= p * math.log(p)\n",
    "    return H\n",
    "\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\n",
    "        \"user_id\",\n",
    "        \"Ru\", \"label\",\n",
    "        \"rating_entropy\",               # (3.2)\n",
    "        \"extremity_ratio\",              # (3.3)\n",
    "        \"average_rating_deviation\",     # (3.4)\n",
    "        \"review_burst_count\",           # (3.5) approx\n",
    "        \"lexical_diversity\",            # (3.6) proxy\n",
    "        \"review_length_discrepancy\"     # (3.7)\n",
    "    ])\n",
    "\n",
    "    for uid in user_n:\n",
    "        n = user_n[uid]\n",
    "\n",
    "        H = entropy(user_r[uid])\n",
    "        ER = user_extreme[uid] / n if n else 0.0\n",
    "        AAD = user_aad_sum[uid] / n if n else 0.0\n",
    "\n",
    "        # burst approx: sum over buckets of (count-1)\n",
    "        BC = 0\n",
    "        for b, c in user_bucket_cnt[uid].items():\n",
    "            if c > 1:\n",
    "                BC += (c - 1)\n",
    "\n",
    "        # LD proxy: mean per-review TTR\n",
    "        LD = user_ttr_sum[uid] / n if n else 0.0\n",
    "\n",
    "        RD = user_rd_sum[uid] / n if n else 0.0\n",
    "\n",
    "        w.writerow([\n",
    "            uid,\n",
    "            user_Ru.get(uid),\n",
    "            user_label.get(uid),\n",
    "            H, ER, AAD, BC, LD, RD\n",
    "        ])\n",
    "\n",
    "print(\"Saved features to:\", OUT_CSV)\n",
    "print(\"Users:\", len(user_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abc6966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (500000, 9)\n",
      "label\n",
      "fake         488886\n",
      "unlabeled      8630\n",
      "genuine        2484\n",
      "Name: count, dtype: int64\n",
      "2 (500000, 9)\n",
      "label\n",
      "fake         489107\n",
      "unlabeled      8202\n",
      "genuine        2691\n",
      "Name: count, dtype: int64\n",
      "3 (500000, 9)\n",
      "label\n",
      "fake         489778\n",
      "unlabeled      7927\n",
      "genuine        2295\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path_features = \"dataset/user_features.csv\"\n",
    "chunks = pd.read_csv(path_features, chunksize=500_000)   # adjust 200k–1M\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(i, chunk.shape)\n",
    "    # example: count labels per chunk\n",
    "    print(chunk[\"label\"].value_counts(dropna=False).head())\n",
    "    if i == 3:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e374232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>Ru</th>\n",
       "      <th>label</th>\n",
       "      <th>rating_entropy</th>\n",
       "      <th>extremity_ratio</th>\n",
       "      <th>average_rating_deviation</th>\n",
       "      <th>review_burst_count</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>review_length_discrepancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>AHEQSWETDO7DWVUGKK2UJJY5TCNA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465991</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869748</td>\n",
       "      <td>16.184633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000001</th>\n",
       "      <td>AGTX7K7SSEHTBWWJX3BVF3MDHHBA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.860329</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>49.315367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000002</th>\n",
       "      <td>AFVVQIWNFAPFCSEUJJN5XRBG2UJA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.022506</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>19.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000003</th>\n",
       "      <td>AF6XMUKHQCPSB664RC5UYJUJIFCA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.964176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.839514</td>\n",
       "      <td>19.815367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000004</th>\n",
       "      <td>AE3AQHJQD654DBKXCAX4IOK2QYUQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fake</td>\n",
       "      <td>0.500402</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.783925</td>\n",
       "      <td>0</td>\n",
       "      <td>0.884675</td>\n",
       "      <td>22.084633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              user_id   Ru label  rating_entropy  \\\n",
       "1000000  AHEQSWETDO7DWVUGKK2UJJY5TCNA  0.0  fake        0.000000   \n",
       "1000001  AGTX7K7SSEHTBWWJX3BVF3MDHHBA  0.0  fake        0.000000   \n",
       "1000002  AFVVQIWNFAPFCSEUJJN5XRBG2UJA  0.0  fake        0.000000   \n",
       "1000003  AF6XMUKHQCPSB664RC5UYJUJIFCA  0.0  fake        0.000000   \n",
       "1000004  AE3AQHJQD654DBKXCAX4IOK2QYUQ  0.0  fake        0.500402   \n",
       "\n",
       "         extremity_ratio  average_rating_deviation  review_burst_count  \\\n",
       "1000000              1.0                  0.465991                   1   \n",
       "1000001              1.0                  2.860329                   0   \n",
       "1000002              1.0                  1.022506                   0   \n",
       "1000003              1.0                  0.964176                   0   \n",
       "1000004              0.8                  0.783925                   0   \n",
       "\n",
       "         lexical_diversity  review_length_discrepancy  \n",
       "1000000           0.869748                  16.184633  \n",
       "1000001           0.777778                  49.315367  \n",
       "1000002           0.932692                  19.500000  \n",
       "1000003           0.839514                  19.815367  \n",
       "1000004           0.884675                  22.084633  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bfa97e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,000,000 users into dict\n",
      "Loaded 2,000,000 users into dict\n",
      "Loaded 3,000,000 users into dict\n",
      "Loaded 4,000,000 users into dict\n",
      "Loaded 5,000,000 users into dict\n",
      "Loaded 6,000,000 users into dict\n",
      "Loaded 7,000,000 users into dict\n",
      "Loaded 8,000,000 users into dict\n",
      "Loaded 9,000,000 users into dict\n",
      "Loaded 10,000,000 users into dict\n",
      "Loaded 11,000,000 users into dict\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reader, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     11\u001b[0m     uid \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 12\u001b[0m     \u001b[43muser2feat\u001b[49m\u001b[43m[\u001b[49m\u001b[43muid\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating_entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating_entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextremity_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextremity_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_rating_deviation\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_rating_deviation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_burst_count\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_burst_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlexical_diversity\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlexical_diversity\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_length_discrepancy\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_length_discrepancy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1_000_000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m users into dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "FEATURES_CSV = \"dataset/user_features.csv\"\n",
    "IN_JSONL = \"dataset/Clothing_Shoes_and_Jewelry_with_labels.jsonl\"\n",
    "OUT_JSONL = \"dataset/Clothing_Shoes_and_Jewelry_with_labels_and_features.jsonl\"\n",
    "\n",
    "# 1) Load user features into memory (DICT)\n",
    "user2feat = {}\n",
    "\n",
    "with open(FEATURES_CSV, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader, 1):\n",
    "        uid = row[\"user_id\"]\n",
    "        user2feat[uid] = (\n",
    "            float(row[\"rating_entropy\"]) if row[\"rating_entropy\"] else None,\n",
    "            float(row[\"extremity_ratio\"]) if row[\"extremity_ratio\"] else None,\n",
    "            float(row[\"average_rating_deviation\"]) if row[\"average_rating_deviation\"] else None,\n",
    "            int(float(row[\"review_burst_count\"])) if row[\"review_burst_count\"] else None,\n",
    "            float(row[\"lexical_diversity\"]) if row[\"lexical_diversity\"] else None,\n",
    "            float(row[\"review_length_discrepancy\"]) if row[\"review_length_discrepancy\"] else None,\n",
    "        )\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Loaded {i:,} users into dict\")\n",
    "\n",
    "print(\"Total users in dict:\", len(user2feat))\n",
    "\n",
    "# 2) Stream JSONL and append features\n",
    "missing = 0\n",
    "with open(IN_JSONL, \"r\", encoding=\"utf-8\") as fin, open(OUT_JSONL, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for i, line in enumerate(fin, 1):\n",
    "        r = json.loads(line)\n",
    "        uid = r.get(\"user_id\")\n",
    "\n",
    "        feats = user2feat.get(uid)\n",
    "        if feats is None:\n",
    "            missing += 1\n",
    "            r[\"rating_entropy\"] = None\n",
    "            r[\"extremity_ratio\"] = None\n",
    "            r[\"average_rating_deviation\"] = None\n",
    "            r[\"review_burst_count\"] = None\n",
    "            r[\"lexical_diversity\"] = None\n",
    "            r[\"review_length_discrepancy\"] = None\n",
    "        else:\n",
    "            H, ER, AAD, BC, LD, RD = feats\n",
    "            r[\"rating_entropy\"] = H\n",
    "            r[\"extremity_ratio\"] = ER\n",
    "            r[\"average_rating_deviation\"] = AAD\n",
    "            r[\"review_burst_count\"] = BC\n",
    "            r[\"lexical_diversity\"] = LD\n",
    "            r[\"review_length_discrepancy\"] = RD\n",
    "\n",
    "        fout.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if i % 1_000_000 == 0:\n",
    "            print(f\"Processed {i:,} reviews | missing users: {missing:,}\")\n",
    "\n",
    "print(\"Saved:\", OUT_JSONL)\n",
    "print(\"Missing-feature rows:\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851325b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
